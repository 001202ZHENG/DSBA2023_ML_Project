{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Machine Learning\n",
    "\n",
    "**Zheng WAN_B00758336**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "\n",
    "**(a) Stochastic gradient descent performs less computation per update than gradient descent.**  \n",
    "**Answer:** True.  \n",
    "**Justification:** The per-update computational complexity of SGD is $O(d)$, while that of GD is $O(nd)$.\n",
    "\n",
    "---\n",
    "**(b) Both PCA and linear regression can be thought of as algorithms for minimizing a sum of squared errors.**  \n",
    "**Answer:** True.  \n",
    "**Justification:** PCA aims to minimize the squared reconstruction error after projecting data onto lower-dimensional space. Linear regression minimizes the squared difference between observed and predicted values.  \n",
    "\n",
    "---\n",
    "**(c) Let $ A \\in \\mathbb{R}^{m \\times n} $ be the matrix representation of our data. Let's assume that we project our data on the $ k $-dimensional space using Principal Component Analysis, where $ k $ equals the rank of $ A $. Then, no loss is incurred in the reconstruction of the data.**  \n",
    "**Answer:** True.  \n",
    "**Justification:** If $ k $ equals the rank of $ A $, then all the variance in the data is captured by the first $ k $ principal components, and no information is lost during projection.  \n",
    "\n",
    "---\n",
    "**(d) Let $ y_i = \\log(x^1_{i} x^2_{i}) + \\varepsilon_i $ be a model, where $ \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2) $ corresponds to Gaussian noise. Then, the maximum likelihood parameters of the model (Î±) can be learned using linear regression.**  \n",
    "**Answer:** True.  \n",
    "**Justification:** By taking the logarithm of both sides, the equation can be transformed into a linear form suitable for linear regression.  \n",
    "\n",
    "---\n",
    "**(e) The eigenvectors of $ AA^T $ and $ A^T A $ are the same.**  \n",
    "**Answer:** False.  \n",
    "**Justification:** Let $ A \\in \\mathbb{R}^{m \\times n} $, the eigenvectors of $ AA^T $ belong to $ \\mathbb{R}^m $ and those of $ A^T A $ belong to $ \\mathbb{R}^n $. \n",
    "They are not the same.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "\n",
    "**Answer to (a):**\n",
    "$$\\begin{align*}\n",
    "(MM^T)^T &= (M^T)^T M^T \\\\\n",
    "&= MM^T\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "(M^TM)^T &= (M^T)^T M^T \\\\\n",
    "&= MM^T\n",
    "\\end{align*}$$\n",
    "\n",
    "Both $ MM^T $ and $M^TM$ are symmetric. $MM^T$ is $m \\times m$ (square) and $M^TM$ is $n \\times n$ (square). They are real if matrix $M$ has real entries.\n",
    "\n",
    "---\n",
    "\n",
    "**Answer to (b):**\n",
    "\n",
    "**Eigenvalues:**\n",
    "Let $ v $ be an eigenvector of $ M^T M $ with eigenvalue $ \\lambda $. This means:\n",
    "$ M^T Mv = \\lambda v $\n",
    "\n",
    "Multiplying both sides of the equation by matrix $ M $, we get:\n",
    "$$ M(M^T Mv) = \\lambda Mv $$\n",
    "$$ MM^T(Mv) = \\lambda Mv $$\n",
    "This shows that $ Mv $ is an eigenvector of $ MM^T $ with the same eigenvalue $ \\lambda $.\n",
    "\n",
    "**Eigenvectors:**\n",
    "While the eigenvalues of $ MM^T $ and $ M^T M $ are the same, their eigenvectors are generally not the same, because they belong to different vector spaces. \n",
    "\n",
    "---\n",
    "\n",
    "**Answer to (c):**\n",
    "Using the SVD decomposition $M = U\\Sigma V^T$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "M^TM &= (U\\Sigma V^T)^T U\\Sigma V^T \\\\\n",
    "&= V\\Sigma^T U^T U\\Sigma V^T \\\\\n",
    "&= V\\Sigma^2 V^T\n",
    "\\end{align*}$$\n",
    "\n",
    "Similarily,we conclude:\n",
    "$$MM^T = U\\Sigma^2 U^T$$\n",
    "\n",
    "---\n",
    "\n",
    "**Answer to (d):**\n",
    "The eigenvalues of $M^TM$ are the squared singular values of $M$. This means that if $\\lambda$ is an eigenvalue of $M^TM$, then $\\sqrt{\\lambda}$ will be a singular value of $M$.\n",
    "\n",
    "In other words,The singular values of $ M $ are the square roots of the eigenvalues of $ MM^T $ and $ M^T M $\n",
    "    $$\\lambda_i(M^T M) = \\lambda_i(MM^T) = \\sigma_i^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "**(a) Formulate the least squares problem in terms of $ U_k $, $ x $, and $ w $.**\n",
    "\n",
    "The projection is given by $ U_k w $. The reconstruction error between the original data point and its projection is $ x - U_k w $. \n",
    "\n",
    "We want to minimize the square of this error:\n",
    "\n",
    "$$\\min_w \\| x - U_k w \\|^2$$\n",
    "\n",
    "This problem resembles a linear regression problem, where $ U_k $ is like the design matrix, and $ w $ are the regression coefficients.\n",
    "\n",
    "---\n",
    "**(b) Show that the solution of the least squares problem is equal to $ U_k^T x $, which is the projection of $ x $ onto the span of the columns of $ U_k $.**\n",
    "\n",
    "From the previous answer, we have:\n",
    "\n",
    "$$ \\min_w \\| x - U_k w \\|^2 $$\n",
    "\n",
    "To find the value of $ w $ that minimizes this expression, differentiate with respect to $ w $ and set the result to zero:\n",
    "\n",
    "$$ \\frac{d}{dw} \\| x - U_k w \\|^2 = 0 $$\n",
    "$$ \\Rightarrow -2U_k^T(x - U_k w) = 0 $$\n",
    "$$ \\Rightarrow U_k^T x - U_k^T U_k w = 0 $$\n",
    "\n",
    "Given that $ U_k $ consists of orthonormal eigenvectors (from PCA), we have:\n",
    "\n",
    "$$ U_k^T U_k = I $$\n",
    "\n",
    "Where $ I $ is the identity matrix in $ \\mathbb{R}^k $. Substituting this in, we get:\n",
    "\n",
    "$ U_k^T x - w = 0 $\n",
    "$ \\Rightarrow w = U_k^T x $\n",
    "\n",
    "Thus, the solution to the least squares problem $ w $ is  $ U_k^T x $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**(A)**\n",
    "\n",
    "The regularization term $ \\lambda $ in logistic regression serves as a trade-off between the complexity of the model and the fitting to the training data.With larger dataset,   less regularization needed since the data itself can offer better generalization. On the other hand, with a smaller dataset, the risk of overfitting increases.\n",
    "The first model is on 100 million users and then re-trained  on smaller dataset:We need more regularization with the smaller dataset to avoid overfitting.\n",
    "As a result, the regularization hyperparameter $ \\lambda_2 $  is expected to be greater than $ \\lambda_1 $ .\n",
    "\n",
    "**1.[True]** while **2,3,4 [False]**  $ \\lambda_2 $ is expected to be greater than $ \\lambda_1 $.  \n",
    "**4.5.[False]** As there's no linear relationship implied between the dataset size and regularization strength.\n",
    "\n",
    "---\n",
    "**(B)**\n",
    "\n",
    "1. **Using polynomial features**: Adding polynomial features captures more complexities, typically decreasing training error.\n",
    "   \n",
    "2. **Using Ridge to reduce model complexity**: Ridge might slightly increase training error by adding constraints but can improve generalization.\n",
    "\n",
    "3. **Using Lasso to encourage sparse coefficients**: Lasso can increase training error by forcing coefficients to zero but can prevent overfitting.\n",
    "\n",
    "4. **Normalizing the data points**: Normalization doesn't change data relationships, hence doesn't increase training error.\n",
    "\n",
    "Correct choices: 1 and 4.\n",
    "\n",
    "---\n",
    "**(C)**\n",
    "\n",
    "1. **The variance of the method decreases if $ \\lambda $ increases enough**: Correct. As $ \\lambda $ increases, the model becomes more regularized, leading to lower complexity and thus lower variance.\n",
    "\n",
    "2. **There might be multiple solutions for $ w^* $**: Incorrect. Regularization ensures a unique solution, especially when $ X $ is not full rank.\n",
    "\n",
    "3. **The bias of the method decreases if $ \\lambda $ increases enough**: Incorrect. As $ \\lambda $ increases, the model becomes more biased towards simpler solutions.\n",
    "\n",
    "4. **$ w^* = X^+Y $, where $ X^+ $ is the pseudoinverse of $ X $**: Incorrect. This is the solution without the regularization term. With regularization, the solution changes.\n",
    "\n",
    "**Correct answer is: 1.**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "**(a)**\n",
    "Given the cost function for ridge regression:\n",
    "$$J(\\theta) = ||y - X\\theta||^2_2 + \\lambda||\\theta||^2_2 $$\n",
    "\n",
    "Compute the gradient of $ J(\\theta) $ with respect to $ \\theta $:\n",
    "$$ \\nabla_{\\theta} J(\\theta) = -2X^T(y - X\\theta) + 2\\lambda\\theta $$\n",
    "\n",
    "To find the minimum of $ J(\\theta) $, set the gradient to zero:\n",
    "$$ -2X^T(y - X\\theta) + 2\\lambda\\theta = 0 $$\n",
    "\n",
    "Rearrange terms:\n",
    "$$ X^T(y - X\\theta) = \\lambda\\theta $$\n",
    "$$ X^Ty - X^TX\\theta = \\lambda\\theta $$\n",
    "$$ X^Ty = (X^TX + \\lambda I)\\theta $$\n",
    "\n",
    "Solve for $\\theta $:\n",
    "$$\\theta = (X^T X + \\lambda I)^{-1} X^T y $$\n",
    "\n",
    "Where:\n",
    "- $ X $ is the design matrix.\n",
    "- $ y $ is the vector of target values.\n",
    "- $ \\lambda $ is the regularization parameter.\n",
    "- $ I $ is the identity matrix.\n",
    "\n",
    "---\n",
    "**(b)** \n",
    "Ridge Regression penalizes large coefficients, which in turn constrains the model complexity. When models have too many features or when features are correlated, least-squares can fit noise, leading to overfitting. The regularization in ridge regression shrinks the coefficients and helps in preventing this overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "**(a)** Likelihood function $L(\\theta; X_1, ..., X_n)$:\n",
    "For a single random variable $X_i$,\n",
    "$$ p(X_i) = \\theta^{X_i} (1 - \\theta)^{1-X_i} $$\n",
    "Given that $X_i$ are i.i.d., the likelihood function is:\n",
    "$$ L(\\theta; X_1, ..., X_n) = \\prod_{i=1}^{n} \\theta^{X_i} (1 - \\theta)^{1-X_i} $$\n",
    "\n",
    "---\n",
    "**(b)** Starting with the likelihood function:\n",
    "$$ L(\\theta; X_1, ..., X_n) = \\prod_{i=1}^{n} \\theta^{X_i} (1 - \\theta)^{1-X_i} $$\n",
    "\n",
    " Apply the natural logarithm to both sides:\n",
    "$$ \\ln L(\\theta) = \\ln \\prod_{i=1}^{n} \\theta^{X_i} (1 - \\theta)^{1-X_i} $$\n",
    "\n",
    "Use the logarithm properties. When taking the logarithm of a product, it becomes a sum of the logarithms:\n",
    "$$ \\ln L(\\theta) = \\sum_{i=1}^{n} \\ln [\\theta^{X_i} (1 - \\theta)^{1-X_i}] $$\n",
    "\n",
    "Separate the terms inside the logarithm and utilize properties of logarithms:\n",
    "$$ \\ln L(\\theta) = \\sum_{i=1}^{n} [X_i \\ln \\theta + (1-X_i) \\ln(1 - \\theta)] $$\n",
    "\n",
    "---\n",
    "**(c)**\n",
    "To find the value of $\\theta$ that maximizes the log-likelihood function, we differentiate it and set it to zero:\n",
    "$$ \\frac{d \\ln L(\\theta)}{d \\theta} = \\sum_{i=1}^{n} \\left( \\frac{X_i}{\\theta} - \\frac{1-X_i}{1-\\theta} \\right) = 0 $$\n",
    "\n",
    "Expanding $\\frac{d \\ln L(\\theta)}{d \\theta}$ and rearranging, we get:\n",
    "$$ \\sum_{i=1}^{n} X_i = \\theta \\sum_{i=1}^{n} (1 - X_i) + \\theta \\sum_{i=1}^{n} X_i $$\n",
    "\n",
    "Further simplifying, we find:\n",
    "$$ \\theta = \\frac{1}{n} \\sum_{i=1}^{n} X_i $$\n",
    "\n",
    "This is consistent with the given $\\hat{\\theta} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, thus completing the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "**(a)**\n",
    "For $ p(x_i = 1|y) $, the Maximum Likelihood Estimation (MLE) is given by:\n",
    "$$ \\frac{\\text{Number of times } x_i = 1 \\text{ when } y \\text{ takes a particular value}}{\\text{Total number of times } y \\text{ takes that particular value}} $$\n",
    "\n",
    "\n",
    "|        | $ y = +1 $ | $ y = 0 $ | $ y = -1 $ |\n",
    "|--------|--------------|-------------|--------------|\n",
    "| $ x_1 = 1 $ | 0.5          | 0.5        | 1/3        |\n",
    "| $ x_2 = 1 $ | 1            | 0.5        | 1/3        |\n",
    "| $ x_3 = 1 $ | 0.5          | 1          | 1/3        |\n",
    "| $ x_4 = 1 $ | 0.5          | 1          | 2/3        |\n",
    "\n",
    "---\n",
    "**(b) Compute the MLE**\n",
    "\n",
    "$$ p(y = k) = \\frac{\\text{Number of times } y = k}{\\text{Total number of data points}} $$\n",
    "\n",
    "Using the above formula:\n",
    "- $ p(y = +1) = 2/7 $\n",
    "- $ p(y = 0) = 2/7 $ \n",
    "- $ p(y = -1) = 3/7 $\n",
    "\n",
    "---\n",
    "**(c)**\n",
    "\n",
    "For naive Bayes, the class assigned to a new data point is the one that maximizes the following:\n",
    "\n",
    "$$p(y=k | \\mathbf{x}) \\propto p(y=k) \\prod_{i=1}^{n} p(x_i | y=k) $$\n",
    "\n",
    "Where $n$ is the number of features, and in this case $n = 4$.\n",
    "\n",
    "We have three classes $y = +1, y = 0$, and $y = -1$. We will calculate the above value for each class and see which class gives the maximum value.\n",
    "\n",
    "1. For $y = +1$:\n",
    "\n",
    "$$ p(y=+1 | \\mathbf{x}) \\propto p(y=+1) \\times p(x_1=1 | y=+1) \\times p(x_2=1 | y=+1) \\times p(x_3=1 | y=+1) \\times p(x_4=1 | y=+1) $$\n",
    "\n",
    "$$ p(y=+1 | \\mathbf{x}) \\propto \\frac{2}{7} \\times 0.5 \\times 1 \\times 0.5 \\times 0.5 = \\frac{1}{28} $$\n",
    "\n",
    "2. For $y = 0$:\n",
    "\n",
    "$$ p(y=0 | \\mathbf{x}) \\propto p(y=0) \\times p(x_1=1 | y=0) \\times p(x_2=1 | y=0) \\times p(x_3=1 | y=0) \\times p(x_4=1 | y=0) $$\n",
    "\n",
    "$$ p(y=0 | \\mathbf{x}) \\propto \\frac{2}{7} \\times 0.5 \\times 0.5 \\times 1 \\times 1 = \\frac{1}{14} $$\n",
    "\n",
    "3. For $y = -1$:\n",
    "\n",
    "$$ p(y=-1 | \\mathbf{x}) \\propto p(y=-1) \\times p(x_1=1 | y=-1) \\times p(x_2=1 | y=-1) \\times p(x_3=1 | y=-1) \\times p(x_4=1 | y=-1) $$\n",
    "\n",
    "$$ p(y=-1 | \\mathbf{x}) \\propto \\frac{3}{7} \\times \\frac{1}{3} \\times \\frac{1}{3} \\times \\frac{1}{3} \\times \\frac{2}{3} = \\frac{2}{189} $$\n",
    "\n",
    "\n",
    "**The maximum value is $\\frac{1}{7}$ which corresponds to $y = 0$.**\n",
    "\n",
    "**Thus, the new data point with the given feature values is classified as class $y = 0$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read JSON\n",
    "path = 'fantasy_100.json'\n",
    "df = pd.read_json(path, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   user_id       100 non-null    object\n",
      " 1   book_id       100 non-null    int64 \n",
      " 2   review_id     100 non-null    object\n",
      " 3   rating        100 non-null    int64 \n",
      " 4   review_text   100 non-null    object\n",
      " 5   date_added    100 non-null    object\n",
      " 6   date_updated  100 non-null    object\n",
      " 7   read_at       100 non-null    object\n",
      " 8   started_at    100 non-null    object\n",
      " 9   n_votes       100 non-null    int64 \n",
      " 10  n_comments    100 non-null    int64 \n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 8.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       4.090000\n",
       "std        1.137914\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        4.000000\n",
       "75%        5.000000\n",
       "max        5.000000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (a) plot the distribution of rating\n",
    "plt.hist(df['rating'], bins=10, edgecolor='black')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_0 (Intercept): 3.983948\n",
      "Theta_1 (Slope): 0.000119\n",
      "Mean Squared Error: 1.265752\n"
     ]
    }
   ],
   "source": [
    "#compute the length of each review and save as a new col\n",
    "df['review_length'] = df['review_text'].apply(len)\n",
    "\n",
    "# Define predictor (X) and target (y)\n",
    "X = df[['review_length']]\n",
    "Y = df['rating']\n",
    "\n",
    "#Train the model\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X,Y)\n",
    "\n",
    "#Prediction\n",
    "predictions = model_1.predict(X)\n",
    "\n",
    "# Compute the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(Y, predictions)\n",
    "\n",
    "#compute the parameters\n",
    "theta_0 = model_1.intercept_\n",
    "theta_1 = model_1.coef_[0]\n",
    "\n",
    "print(f\"Theta_0 (Intercept): {round(theta_0,6)}\")\n",
    "print(f\"Theta_1 (Slope): {round(theta_1,6)}\")\n",
    "print(f\"Mean Squared Error: {round(mse,6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation: It is obvious that the length of the review has no impact over the rating of a movie**\n",
    "\n",
    "The values are as follows:\n",
    "- $ \\Theta_0 $ (Intercept): 3.983948\n",
    "- $ \\Theta_1 $ (Slope): 0.000119\n",
    "- Mean Squared Error (MSE): 1.265752\n",
    "\n",
    "Interpretation:\n",
    "- $\\Theta_0$ :if a review had a length of 0 (which isn't practically possible for a genuine review), the predicted rating would be approximately 4.\n",
    "  \n",
    "- $ \\Theta_1 $ (Slope) indicates the change in the predicted 'rating' for a unit increase in 'review_length'.This value is very small, suggesting that the length of the review has a minimal effect on the rating.(Almost no impact)\n",
    "  \n",
    "- The MSE is a measure of how well the model's predictions match the actual values. A smaller MSE indicates a better fit of the model to the data. In this context, the MSE provides a quantified measure of the average squared difference between the predicted ratings and the actual ratings.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_0 (Intercept): 3.954321\n",
      "Theta_1 (Coefficient for review length): 7.2e-05\n",
      "Theta_2 (Coefficient for number of comments): 0.108071\n",
      "Mean Squared Error: 1.265752\n"
     ]
    }
   ],
   "source": [
    "# Define predictor (X1) (X2) and target (Y)\n",
    "#Y = df['rating']\n",
    "X2 = df[['review_length', 'n_comments']]\n",
    "Y2 = df['rating']\n",
    "\n",
    "model_2 = LinearRegression()\n",
    "model_2.fit(X2,Y2)\n",
    "\n",
    "predictions = model_2.predict(X2)\n",
    "\n",
    "mse2 = mean_squared_error(Y2, predictions)\n",
    "\n",
    "theta0 = model_2.intercept_\n",
    "theta1, theta2 = model_2.coef_\n",
    "\n",
    "print(f\"Theta_0 (Intercept): {round(theta0,6)}\")\n",
    "print(f\"Theta_1 (Coefficient for review length): {round(theta1,6)}\")\n",
    "print(f\"Theta_2 (Coefficient for number of comments): {round(theta2,6)}\")\n",
    "print(f\"Mean Squared Error: {round(mse,6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- The change in $ \\Theta_1 $ when introducing the number of comments as an additional predictor is due to the shared variance between review length and the number of comments and the model's attempt to attribute the explained variance in the star rating to each predictor effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.423\n"
     ]
    }
   ],
   "source": [
    "# Sample data (Replace this with your data)\n",
    "# X - Feature matrix with columns 'review length' and 'number of comments'\n",
    "# y - Target vector (e.g., star rating)\n",
    "X = df[['review_length', 'n_comments']].values\n",
    "y = df['rating'].values\n",
    "\n",
    "# Polynomial feature expansion function\n",
    "def polynomial_expansion(X):\n",
    "    x1 = X[:, 0]  # review length\n",
    "    x2 = X[:, 1]  # number of comments\n",
    "    \n",
    "    # Calculate polynomial features\n",
    "    x1_squared = x1**2\n",
    "    x2_squared = x2**2\n",
    "    interaction = x1 * x2\n",
    "    \n",
    "    # Create new feature matrix\n",
    "    X_poly = np.column_stack((x1, x2, x1_squared, x2_squared, interaction))\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "# Transform the original features\n",
    "X_poly = polynomial_expansion(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and MSE\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", round(mse,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Vector**:\n",
    "1. Intercept term: $ 1 $ (this is to account for the bias term, $ \\Theta_0 $)\n",
    "2. Original `review length`: $ x_1 $\n",
    "3. Original `number of comments`: $ x_2 $\n",
    "4. Square of `review length`: $ x_1^2 $\n",
    "5. Square of `number of comments`: $ x_2^2 $\n",
    "6. Interaction term between `review length` and `number of comments`: $ x_1 \\times x_2 $\n",
    "\n",
    "polynomial feature vector: \n",
    "$$ \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_2^2 \\\\ x_1 \\times x_2 \\end{bmatrix} $$\n",
    "\n",
    "This vector now contains the original features, their squares, and the interaction term, leading to a more flexible model that can capture non-linear relationships and interactions between the features.\n",
    "\n",
    "**Mean Squared Error (MSE)**: 1.423\n",
    "\n",
    "This is the average squared difference between the observed outcomes and the model's predictions.\n",
    "\n",
    "**Observations**:\n",
    "1. **Complexity**: Introducing polynomial features increases the complexity of the model. This allows the model to fit more complex, non-linear relationships in the data.\n",
    "2. **Risk of Overfitting**: While polynomial regression can capture non-linear patterns, it also comes with an increased risk of overfitting, especially with high-degree polynomials. \n",
    "3. **Interactions**: The interaction term $ x_1 \\times x_2 $ captures the combined effect of `review length` and `number of comments` on the star rating. For instance, it helps understand scenarios where longer reviews with many comments have a different impact on the star rating than what would be predicted by considering each feature independently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
